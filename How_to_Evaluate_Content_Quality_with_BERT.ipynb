{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Evaluate Content Quality with BERT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGHBKLw8u0XO/bp+BzBmWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamletbatista/sel/blob/master/How_to_Evaluate_Content_Quality_with_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVtwG-jNxJKF",
        "colab_type": "text"
      },
      "source": [
        "#How to Evalute Content Quality with BERT\n",
        "\n",
        "1. Build a predictive model to classify grammatically correct sentences\n",
        "2. Fetch a target page and extract the text.\n",
        "3. Split it into sentences.\n",
        "4. Predict each sentence as gramatically correct or not.\n",
        "5. Calculate and report the percentage of gramatically correct sentences\n",
        "\n",
        "We will use Ludwig to train a BERT text classification model on the Corpus of Linguistic Acceptability (CoLA) dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZETwQbj4iIq",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "url=\"https://searchengineland.com/the-dangers-of-misplaced-third-party-scripts-327329\" #@param {type:\"string\"}\n",
        "selector=\"p > a\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DECyWyyt4rID",
        "colab_type": "text"
      },
      "source": [
        "##Build Predictive Model\n",
        "Sourced from https://colab.research.google.com/drive/13ErkLg5FZHIbnUGZRkKlL-9WNCNQPIow#scrollTo=RYZgdpzpwY6w"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj2ft0Gq40G6",
        "colab_type": "code",
        "outputId": "3c8b8d25-c167-4c45-c3b6-91e903214760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://nyu-mll.github.io/CoLA/cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 18:28:13--  https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
            "Resolving nyu-mll.github.io (nyu-mll.github.io)... 185.199.108.153, 185.199.110.153, 185.199.109.153, ...\n",
            "Connecting to nyu-mll.github.io (nyu-mll.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255330 (249K) [application/zip]\n",
            "Saving to: ‘cola_public_1.1.zip’\n",
            "\n",
            "\rcola_public_1.1.zip   0%[                    ]       0  --.-KB/s               \rcola_public_1.1.zip 100%[===================>] 249.35K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-06-06 18:28:13 (7.29 MB/s) - ‘cola_public_1.1.zip’ saved [255330/255330]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFsWFhB_49L1",
        "colab_type": "code",
        "outputId": "f6c01865-2f45-4ce4-c77f-50048ffb42af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LnY6W0v5AWJ",
        "colab_type": "code",
        "outputId": "fd1965ff-3e5c-4cf0-ed63-5de0f7adf319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nora sent the book from Paris to London.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4883</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This is the student pictures of whom appeared ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Clearly, John perfectly will immediately learn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5186</th>\n",
              "      <td>kl93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Perhaps some dry socks would help?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6964</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Emma and Harriet were attacked by those bandits.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Louise likes not being happy, doesn't she?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>r-67</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They can't stand each other, them.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4094</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The picture on the wall reminded him of his co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4396</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mary did not avoid Bill.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4878</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The president Fred voted for has resigned.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2669            l-93  ...           Nora sent the book from Paris to London.\n",
              "4883            ks08  ...  This is the student pictures of whom appeared ...\n",
              "694             bc01  ...  Clearly, John perfectly will immediately learn...\n",
              "5186            kl93  ...                 Perhaps some dry socks would help?\n",
              "6964            m_02  ...   Emma and Harriet were attacked by those bandits.\n",
              "326             bc01  ...         Louise likes not being happy, doesn't she?\n",
              "1847            r-67  ...                 They can't stand each other, them.\n",
              "4094            ks08  ...  The picture on the wall reminded him of his co...\n",
              "4396            ks08  ...                           Mary did not avoid Bill.\n",
              "4878            ks08  ...         The president Fred voted for has resigned.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_RqFkAAF3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save to CSV\n",
        "df.to_csv(\"cola_dataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28v6m90y5S3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49GeF6EX5lgh",
        "colab_type": "text"
      },
      "source": [
        "### Create Ludwig Model Definition\n",
        "\n",
        "Sourced from https://gist.github.com/hamletbatista/f5993ee38d14643f0df71ae2303f5dfa#file-bert_model_definition-py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlDHZEXH5ps7",
        "colab_type": "code",
        "outputId": "012b93cb-aade-43d1-e5ce-2eb1e203297e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf; print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKZA1ggI6JUe",
        "colab_type": "code",
        "outputId": "3d394351-5340-4c41-9e41-ec64707ce8e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "source": [
        "#https://github.com/uber/ludwig/blob/master/requirements.txt\n",
        "#requires tensorflow 1.15.3\n",
        "\n",
        "!pip install tensorflow-gpu==1.15.3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.3\n",
            "  Using cached https://files.pythonhosted.org/packages/98/ab/19aba3629427c2d96790f73838639136ce02b6e7e1c4f2dd60149174c794/tensorflow_gpu-1.15.3-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.29.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.18.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (3.2.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (0.9.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.3) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.3) (47.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.3) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.3) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.3) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.3) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.3) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "  Found existing installation: tensorflow-gpu 2.2.0\n",
            "    Uninstalling tensorflow-gpu-2.2.0:\n",
            "      Successfully uninstalled tensorflow-gpu-2.2.0\n",
            "Successfully installed tensorflow-gpu-1.15.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b46MHIPR9Ksp",
        "colab_type": "code",
        "outputId": "cc76502a-43cd-4cb0-8973-a3cc14d8beb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf; print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwK_A1VX6yiz",
        "colab_type": "code",
        "outputId": "b438e057-078d-4460-8ee0-27e2e97d4468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install ludwig"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ludwig in /usr/local/lib/python3.6/dist-packages (0.2.2.7)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.8.7)\n",
            "Requirement already satisfied: h5py>=2.6 in /usr/local/lib/python3.6/dist-packages (from ludwig) (2.10.0)\n",
            "Requirement already satisfied: tensorflow==1.15.3 in /usr/local/lib/python3.6/dist-packages (from ludwig) (1.15.3)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from ludwig) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ludwig) (4.41.1)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.6/dist-packages (from ludwig) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from ludwig) (1.18.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from ludwig) (1.0.4)\n",
            "Requirement already satisfied: Cython>=0.25 in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.29.19)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.6->ludwig) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15.3->ludwig) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15.3->ludwig) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (1.29.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.3->ludwig) (1.12.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ludwig) (0.15.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->ludwig) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->ludwig) (2.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3->ludwig) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3->ludwig) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3->ludwig) (47.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3->ludwig) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3->ludwig) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t41KnmiY6_Ck",
        "colab_type": "code",
        "outputId": "67accf55-3e5e-460b-a766-775a34665deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 18:47:51--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   127MB/s    in 3.1s    \n",
            "\n",
            "2020-06-06 18:47:54 (127 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq0BxE-a9cun",
        "colab_type": "code",
        "outputId": "71fa0010-ea99-4957-82cc-a80a44ee7bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_oK1kI9-bDA",
        "colab_type": "text"
      },
      "source": [
        "### Get Appropiate Hyperparameters\n",
        "Sourced from https://app.wandb.ai/cayush/bert-finetuning/reports/Sentence-classification-with-Huggingface-BERT-and-W%26B--Vmlldzo4MDMwNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTC52b5K9zHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://uber.github.io/ludwig/user_guide/#bert-encoder\n",
        "\n",
        "template=\"\"\"\n",
        "input_features:\n",
        "    -\n",
        "        name: sentence\n",
        "        type: text\n",
        "        encoder: bert\n",
        "        config_path: uncased_L-12_H-768_A-12/bert_config.json\n",
        "        checkpoint_path: uncased_L-12_H-768_A-12/bert_model.ckpt\n",
        "        preprocessing:\n",
        "          word_tokenizer: bert\n",
        "          word_vocab_file: uncased_L-12_H-768_A-12/vocab.txt\n",
        "          padding_symbol: '[PAD]'\n",
        "          unknown_symbol: '[UNK]'\n",
        "\n",
        "output_features:\n",
        "    -\n",
        "        name: label\n",
        "        type: category\n",
        "text:\n",
        "        word_sequence_length_limit: 128\n",
        "training:\n",
        "        batch_size: 16\n",
        "        learning_rate: 0.00003\n",
        "        epochs: 3\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"model_definition.yaml\", \"w\") as f:\n",
        "  f.write(template)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v85zhAWz_emS",
        "colab_type": "code",
        "outputId": "7df5e068-9182-4366-8446-f0524a5cbd8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cola_public\t     model_definition.yaml  uncased_L-12_H-768_A-12\n",
            "cola_public_1.1.zip  sample_data\t    uncased_L-12_H-768_A-12.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zNkVFvw_ikg",
        "colab_type": "code",
        "outputId": "0b53eb19-3114-4c1b-835d-fac4e7c4f641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 28.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EclhPHigbukK",
        "colab_type": "text"
      },
      "source": [
        "### Train the predictive model\n",
        "\n",
        "Sourced from https://ludwig-ai.github.io/ludwig-docs/examples/#text-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eodz2NpN_1Hw",
        "colab_type": "code",
        "outputId": "b8940f07-87f4-4e35-f5cf-f2bbb2980f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ludwig experiment --data_csv cola_dataset.csv --model_definition_file model_definition.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "███████████████████████\n",
            "█ █ █ █  ▜█ █ █ █ █   █\n",
            "█ █ █ █ █ █ █ █ █ █ ███\n",
            "█ █   █ █ █ █ █ █ █ ▌ █\n",
            "█ █████ █ █ █ █ █ █ █ █\n",
            "█     █  ▟█     █ █   █\n",
            "███████████████████████\n",
            "ludwig v0.2.2.7 - Experiment\n",
            "\n",
            "Experiment name: experiment\n",
            "Model name: run\n",
            "Output path: results/experiment_run\n",
            "\n",
            "\n",
            "ludwig_version: '0.2.2.7'\n",
            "command: ('/usr/local/bin/ludwig experiment --data_csv cola_dataset.csv '\n",
            " '--model_definition_file model_definition.yaml')\n",
            "random_seed: 42\n",
            "input_data: 'cola_dataset.csv'\n",
            "model_definition: {   'combiner': {'type': 'concat'},\n",
            "    'input_features': [   {   'checkpoint_path': 'uncased_L-12_H-768_A-12/bert_model.ckpt',\n",
            "                              'config_path': 'uncased_L-12_H-768_A-12/bert_config.json',\n",
            "                              'encoder': 'bert',\n",
            "                              'level': 'word',\n",
            "                              'name': 'sentence',\n",
            "                              'preprocessing': {   'padding_symbol': '[PAD]',\n",
            "                                                   'unknown_symbol': '[UNK]',\n",
            "                                                   'word_tokenizer': 'bert',\n",
            "                                                   'word_vocab_file': 'uncased_L-12_H-768_A-12/vocab.txt'},\n",
            "                              'tied_weights': None,\n",
            "                              'type': 'text'}],\n",
            "    'output_features': [   {   'dependencies': [],\n",
            "                               'loss': {   'class_similarities_temperature': 0,\n",
            "                                           'class_weights': 1,\n",
            "                                           'confidence_penalty': 0,\n",
            "                                           'distortion': 1,\n",
            "                                           'labels_smoothing': 0,\n",
            "                                           'negative_samples': 0,\n",
            "                                           'robust_lambda': 0,\n",
            "                                           'sampler': None,\n",
            "                                           'type': 'softmax_cross_entropy',\n",
            "                                           'unique': False,\n",
            "                                           'weight': 1},\n",
            "                               'name': 'label',\n",
            "                               'reduce_dependencies': 'sum',\n",
            "                               'reduce_input': 'sum',\n",
            "                               'top_k': 3,\n",
            "                               'type': 'category'}],\n",
            "    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\n",
            "                                      'audio_file_length_limit_in_s': 7.5,\n",
            "                                      'in_memory': True,\n",
            "                                      'missing_value_strategy': 'backfill',\n",
            "                                      'norm': None,\n",
            "                                      'padding_value': 0},\n",
            "                         'bag': {   'fill_value': '<UNK>',\n",
            "                                    'lowercase': False,\n",
            "                                    'missing_value_strategy': 'fill_with_const',\n",
            "                                    'most_common': 10000,\n",
            "                                    'tokenizer': 'space'},\n",
            "                         'binary': {   'fill_value': 0,\n",
            "                                       'missing_value_strategy': 'fill_with_const'},\n",
            "                         'category': {   'fill_value': '<UNK>',\n",
            "                                         'lowercase': False,\n",
            "                                         'missing_value_strategy': 'fill_with_const',\n",
            "                                         'most_common': 10000},\n",
            "                         'date': {   'datetime_format': None,\n",
            "                                     'fill_value': '',\n",
            "                                     'missing_value_strategy': 'fill_with_const'},\n",
            "                         'force_split': False,\n",
            "                         'h3': {   'fill_value': 576495936675512319,\n",
            "                                   'missing_value_strategy': 'fill_with_const'},\n",
            "                         'image': {   'in_memory': True,\n",
            "                                      'missing_value_strategy': 'backfill',\n",
            "                                      'num_processes': 1,\n",
            "                                      'resize_method': 'interpolate',\n",
            "                                      'scaling': 'pixel_normalization'},\n",
            "                         'numerical': {   'fill_value': 0,\n",
            "                                          'missing_value_strategy': 'fill_with_const',\n",
            "                                          'normalization': None},\n",
            "                         'sequence': {   'fill_value': '<UNK>',\n",
            "                                         'lowercase': False,\n",
            "                                         'missing_value_strategy': 'fill_with_const',\n",
            "                                         'most_common': 20000,\n",
            "                                         'padding': 'right',\n",
            "                                         'padding_symbol': '<PAD>',\n",
            "                                         'sequence_length_limit': 256,\n",
            "                                         'tokenizer': 'space',\n",
            "                                         'unknown_symbol': '<UNK>',\n",
            "                                         'vocab_file': None},\n",
            "                         'set': {   'fill_value': '<UNK>',\n",
            "                                    'lowercase': False,\n",
            "                                    'missing_value_strategy': 'fill_with_const',\n",
            "                                    'most_common': 10000,\n",
            "                                    'tokenizer': 'space'},\n",
            "                         'split_probabilities': (0.7, 0.1, 0.2),\n",
            "                         'stratify': None,\n",
            "                         'text': {   'char_most_common': 70,\n",
            "                                     'char_sequence_length_limit': 1024,\n",
            "                                     'char_tokenizer': 'characters',\n",
            "                                     'char_vocab_file': None,\n",
            "                                     'fill_value': '<UNK>',\n",
            "                                     'lowercase': True,\n",
            "                                     'missing_value_strategy': 'fill_with_const',\n",
            "                                     'padding': 'right',\n",
            "                                     'padding_symbol': '<PAD>',\n",
            "                                     'unknown_symbol': '<UNK>',\n",
            "                                     'word_most_common': 20000,\n",
            "                                     'word_sequence_length_limit': 256,\n",
            "                                     'word_tokenizer': 'space_punct',\n",
            "                                     'word_vocab_file': None},\n",
            "                         'timeseries': {   'fill_value': '',\n",
            "                                           'missing_value_strategy': 'fill_with_const',\n",
            "                                           'padding': 'right',\n",
            "                                           'padding_value': 0,\n",
            "                                           'timeseries_length_limit': 256,\n",
            "                                           'tokenizer': 'space'},\n",
            "                         'vector': {   'fill_value': '',\n",
            "                                       'missing_value_strategy': 'fill_with_const'}},\n",
            "    'text': {'word_sequence_length_limit': 128},\n",
            "    'training': {   'batch_size': 16,\n",
            "                    'bucketing_field': None,\n",
            "                    'decay': False,\n",
            "                    'decay_rate': 0.96,\n",
            "                    'decay_steps': 10000,\n",
            "                    'dropout_rate': 0.0,\n",
            "                    'early_stop': 5,\n",
            "                    'epochs': 3,\n",
            "                    'eval_batch_size': 0,\n",
            "                    'gradient_clipping': None,\n",
            "                    'increase_batch_size_on_plateau': 0,\n",
            "                    'increase_batch_size_on_plateau_max': 512,\n",
            "                    'increase_batch_size_on_plateau_patience': 5,\n",
            "                    'increase_batch_size_on_plateau_rate': 2,\n",
            "                    'learning_rate': 3e-05,\n",
            "                    'learning_rate_warmup_epochs': 1,\n",
            "                    'optimizer': {   'beta1': 0.9,\n",
            "                                     'beta2': 0.999,\n",
            "                                     'epsilon': 1e-08,\n",
            "                                     'type': 'adam'},\n",
            "                    'reduce_learning_rate_on_plateau': 0,\n",
            "                    'reduce_learning_rate_on_plateau_patience': 5,\n",
            "                    'reduce_learning_rate_on_plateau_rate': 0.5,\n",
            "                    'regularization_lambda': 0,\n",
            "                    'regularizer': 'l2',\n",
            "                    'staircase': False,\n",
            "                    'validation_field': 'combined',\n",
            "                    'validation_measure': 'loss'}}\n",
            "\n",
            "\n",
            "Using full raw csv, no hdf5 and json file with the same name have been found\n",
            "Building dataset (it may take a while)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Writing dataset\n",
            "Writing train set metadata with vocabulary\n",
            "Training set: 6071\n",
            "Validation set: 788\n",
            "Test set: 1692\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/sequence_encoders.py:1731: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/sequence_encoders.py:1742: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/sequence_encoders.py:1749: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            "╒══════════╕\n",
            "│ TRAINING │\n",
            "╘══════════╛\n",
            "\n",
            "2020-06-06 19:01:48.270332: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-06-06 19:01:48.275965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-06-06 19:01:48.276248: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29d39c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-06 19:01:48.276286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-06-06 19:01:48.281008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-06-06 19:01:48.454274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 19:01:48.455322: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29d3800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-06 19:01:48.455355: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-06-06 19:01:48.456708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 19:01:48.457295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-06 19:01:48.478593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-06-06 19:01:48.692014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-06-06 19:01:48.779878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-06-06 19:01:48.811296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-06-06 19:01:49.041494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-06-06 19:01:49.196072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-06-06 19:01:49.671683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-06 19:01:49.671895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 19:01:49.672654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 19:01:49.673233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-06 19:01:49.673322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-06-06 19:01:49.674977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-06 19:01:49.675020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-06-06 19:01:49.675033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-06-06 19:01:49.675205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 19:01:49.675857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-06 19:01:49.676427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "\n",
            "Epoch 1\n",
            "Training:   0% 0/380 [00:00<?, ?it/s]2020-06-06 19:01:59.676491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "Training: 100% 380/380 [00:45<00:00,  8.27it/s]\n",
            "Evaluation train: 100% 380/380 [00:14<00:00, 26.51it/s]\n",
            "Evaluation vali : 100% 50/50 [00:01<00:00, 27.50it/s]\n",
            "Evaluation test : 100% 106/106 [00:03<00:00, 27.54it/s]\n",
            "Took 1m 6.1650s\n",
            "╒══════════╤════════╤════════════╤═════════════╕\n",
            "│ label    │   loss │   accuracy │   hits_at_k │\n",
            "╞══════════╪════════╪════════════╪═════════════╡\n",
            "│ training │ 0.3888 │     0.8417 │      1.0000 │\n",
            "├──────────┼────────┼────────────┼─────────────┤\n",
            "│ vali     │ 0.4611 │     0.7817 │      1.0000 │\n",
            "├──────────┼────────┼────────────┼─────────────┤\n",
            "│ test     │ 0.4440 │     0.8091 │      1.0000 │\n",
            "╘══════════╧════════╧════════════╧═════════════╛\n",
            "Validation loss on combined improved, model saved\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Training: 100% 380/380 [00:40<00:00,  9.28it/s]\n",
            "Evaluation train: 100% 380/380 [00:13<00:00, 27.71it/s]\n",
            "Evaluation vali : 100% 50/50 [00:01<00:00, 27.94it/s]\n",
            "Evaluation test : 100% 106/106 [00:03<00:00, 27.73it/s]\n",
            "Took 1m 0.2874s\n",
            "╒══════════╤════════╤════════════╤═════════════╕\n",
            "│ label    │   loss │   accuracy │   hits_at_k │\n",
            "╞══════════╪════════╪════════════╪═════════════╡\n",
            "│ training │ 0.1773 │     0.9343 │      1.0000 │\n",
            "├──────────┼────────┼────────────┼─────────────┤\n",
            "│ vali     │ 0.5153 │     0.7919 │      1.0000 │\n",
            "├──────────┼────────┼────────────┼─────────────┤\n",
            "│ test     │ 0.4546 │     0.8097 │      1.0000 │\n",
            "╘══════════╧════════╧════════════╧═════════════╛\n",
            "Last improvement of loss on combined happened 1 epoch ago\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Training: 100% 380/380 [00:40<00:00,  9.27it/s]\n",
            "Evaluation train: 100% 380/380 [00:13<00:00, 27.66it/s]\n",
            "Evaluation vali : 100% 50/50 [00:01<00:00, 27.83it/s]\n",
            "Evaluation test : 100% 106/106 [00:03<00:00, 27.70it/s]\n",
            "Took 1m 0.3476s\n",
            "╒══════════╤════════╤════════════╤═════════════╕\n",
            "│ label    │   loss │   accuracy │   hits_at_k │\n",
            "╞══════════╪════════╪════════════╪═════════════╡\n",
            "│ training │ 0.1332 │     0.9503 │      1.0000 │\n",
            "├──────────┼────────┼────────────┼─────────────┤\n",
            "│ vali     │ 0.6374 │     0.7944 │      1.0000 │\n",
            "├──────────┼────────┼────────────┼─────────────┤\n",
            "│ test     │ 0.5843 │     0.7943 │      1.0000 │\n",
            "╘══════════╧════════╧════════════╧═════════════╛\n",
            "Last improvement of loss on combined happened 2 epochs ago\n",
            "\n",
            "Best validation model epoch: 1\n",
            "Best validation model loss on validation set combined: 0.4611173261543216\n",
            "Best validation model loss on test set combined: 0.4440391896464301\n",
            "\n",
            "Finished: experiment_run\n",
            "Saved to: results/experiment_run\n",
            "\n",
            "╒═════════╕\n",
            "│ PREDICT │\n",
            "╘═════════╛\n",
            "\n",
            "Evaluation: 100% 106/106 [00:04<00:00, 23.59it/s]\n",
            "\n",
            "===== label =====\n",
            "accuracy: 0.8002364066193853\n",
            "hits_at_k: 1.0\n",
            "loss: 0.575182903760573\n",
            "overall_stats: { 'avg_f1_score_macro': 0.7710628458700552,\n",
            "  'avg_f1_score_micro': 0.8002364066193853,\n",
            "  'avg_f1_score_weighted': 0.8055494723187666,\n",
            "  'avg_precision_macro': 0.7608716570924092,\n",
            "  'avg_precision_micro': 0.8002364066193853,\n",
            "  'avg_precision_weighted': 0.8002364066193853,\n",
            "  'avg_recall_macro': 0.7903350009434491,\n",
            "  'avg_recall_micro': 0.8002364066193853,\n",
            "  'avg_recall_weighted': 0.8002364066193853,\n",
            "  'kappa_score': 0.5443328441191515,\n",
            "  'token_accuracy': 0.8002364066193853}\n",
            "per_class_stats: {<UNK>: {   'accuracy': 1.0,\n",
            "    'f1_score': 0,\n",
            "    'fall_out': 0.0,\n",
            "    'false_discovery_rate': 1.0,\n",
            "    'false_negative_rate': 1.0,\n",
            "    'false_negatives': 0,\n",
            "    'false_omission_rate': 0.0,\n",
            "    'false_positive_rate': 0.0,\n",
            "    'false_positives': 0,\n",
            "    'hit_rate': 0,\n",
            "    'informedness': 0.0,\n",
            "    'markedness': 0.0,\n",
            "    'matthews_correlation_coefficient': 0,\n",
            "    'miss_rate': 1.0,\n",
            "    'negative_predictive_value': 1.0,\n",
            "    'positive_predictive_value': 0,\n",
            "    'precision': 0,\n",
            "    'recall': 0,\n",
            "    'sensitivity': 0,\n",
            "    'specificity': 1.0,\n",
            "    'true_negative_rate': 1.0,\n",
            "    'true_negatives': 1692,\n",
            "    'true_positive_rate': 0,\n",
            "    'true_positives': 0},\n",
            "  1: {   'accuracy': 0.8002364066193853,\n",
            "    'f1_score': 0.852787456445993,\n",
            "    'fall_out': 0.2331288343558282,\n",
            "    'false_discovery_rate': 0.10430009149130837,\n",
            "    'false_negative_rate': 0.18620116375727347,\n",
            "    'false_negatives': 224,\n",
            "    'false_omission_rate': 0.37395659432387307,\n",
            "    'false_positive_rate': 0.2331288343558282,\n",
            "    'false_positives': 114,\n",
            "    'hit_rate': 0.8137988362427265,\n",
            "    'informedness': 0.5806700018868982,\n",
            "    'markedness': 0.5217433141848185,\n",
            "    'matthews_correlation_coefficient': 0.5504186508760174,\n",
            "    'miss_rate': 0.18620116375727347,\n",
            "    'negative_predictive_value': 0.6260434056761269,\n",
            "    'positive_predictive_value': 0.8956999085086916,\n",
            "    'precision': 0.8956999085086916,\n",
            "    'recall': 0.8137988362427265,\n",
            "    'sensitivity': 0.8137988362427265,\n",
            "    'specificity': 0.7668711656441718,\n",
            "    'true_negative_rate': 0.7668711656441718,\n",
            "    'true_negatives': 375,\n",
            "    'true_positive_rate': 0.8137988362427265,\n",
            "    'true_positives': 979},\n",
            "  0: {   'accuracy': 0.8002364066193853,\n",
            "    'f1_score': 0.6893382352941176,\n",
            "    'fall_out': 0.18620116375727347,\n",
            "    'false_discovery_rate': 0.37395659432387307,\n",
            "    'false_negative_rate': 0.2331288343558282,\n",
            "    'false_negatives': 114,\n",
            "    'false_omission_rate': 0.10430009149130837,\n",
            "    'false_positive_rate': 0.18620116375727347,\n",
            "    'false_positives': 224,\n",
            "    'hit_rate': 0.7668711656441718,\n",
            "    'informedness': 0.5806700018868982,\n",
            "    'markedness': 0.5217433141848185,\n",
            "    'matthews_correlation_coefficient': 0.5504186508760174,\n",
            "    'miss_rate': 0.2331288343558282,\n",
            "    'negative_predictive_value': 0.8956999085086916,\n",
            "    'positive_predictive_value': 0.6260434056761269,\n",
            "    'precision': 0.6260434056761269,\n",
            "    'recall': 0.7668711656441718,\n",
            "    'sensitivity': 0.7668711656441718,\n",
            "    'specificity': 0.8137988362427265,\n",
            "    'true_negative_rate': 0.8137988362427265,\n",
            "    'true_negatives': 979,\n",
            "    'true_positive_rate': 0.7668711656441718,\n",
            "    'true_positives': 375}}\n",
            "\n",
            "Finished: experiment_run\n",
            "Saved to: results/experiment_run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6boPJY8EUS6",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "Sourced from https://ludwig-ai.github.io/ludwig-docs/getting_started/#programmatic-api\n",
        "\n",
        "Using cola_public/raw/out_of_domain_dev.tsv  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pazth4QfAgC5",
        "colab_type": "code",
        "outputId": "98c8fc38-a7b8-42ef-e674-347d3c3f3218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "from ludwig.api import LudwigModel\n",
        "\n",
        "model = LudwigModel.load(\"results/experiment_run/model\")\n",
        "\n",
        "test_df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "#we rename Query to Questions to match what the model expects\n",
        "predictions = model.predict(test_df)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/sequence_encoders.py:1731: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/sequence_encoders.py:1742: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/sequence_encoders.py:1749: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Restoring parameters from results/experiment_run/model/model_weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoqhx9AiFD3D",
        "colab_type": "code",
        "outputId": "d7a00077-25e3-4567-e16f-79c9df84e08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_df.join(predictions)[[\"sentence\", \"label_predictions\"]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Somebody just left - guess who.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>They claimed they had settled on something, bu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If Sam was going, Sally would know where.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>They're going to serve the guests something, b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>She's reading. I can't imagine what.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>John considers Bill silly.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>John considers Bill to be silly.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>John bought a dog for himself to play with.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>John arranged for himself to get the prize.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>John talked to Bill about himself.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>516 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence label_predictions\n",
              "0                      Somebody just left - guess who.                 1\n",
              "1    They claimed they had settled on something, bu...                 1\n",
              "2            If Sam was going, Sally would know where.                 1\n",
              "3    They're going to serve the guests something, b...                 1\n",
              "4                 She's reading. I can't imagine what.                 1\n",
              "..                                                 ...               ...\n",
              "511                         John considers Bill silly.                 1\n",
              "512                   John considers Bill to be silly.                 1\n",
              "513        John bought a dog for himself to play with.                 1\n",
              "514        John arranged for himself to get the prize.                 1\n",
              "515                 John talked to Bill about himself.                 0\n",
              "\n",
              "[516 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoGm7HvqGluh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = test_df.join(predictions)[[\"sentence\", \"label_predictions\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ct5gj-uG24E",
        "colab_type": "code",
        "outputId": "96bedc7f-92d9-4ea5-da93-5a0eb04e665a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "pred_df.groupby(\"label_predictions\").count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label_predictions</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   sentence\n",
              "label_predictions          \n",
              "0                        92\n",
              "1                       424"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3xEUshgG7Yp",
        "colab_type": "code",
        "outputId": "06cce6ff-5ab9-49e3-e30b-e5c3efd4c817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#pred_df[pred_df.label_predictions != 0]\n",
        "pred_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 516 entries, 0 to 515\n",
            "Data columns (total 2 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   sentence           516 non-null    object\n",
            " 1   label_predictions  516 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 8.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHVydYMOHxpP",
        "colab_type": "code",
        "outputId": "edde618f-1bdb-49f5-8f3b-1316f10986b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "pred_df[pd.to_numeric(pred_df.label_predictions) == 0]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>She knew French for Tom.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>She was dancing with somebody, but I don't kno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>I think Agnes said that Bill would speak, but ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Who did they see someone?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>The book was by John written.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>That John is reluctant seems.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>It is to give up to leave.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>I presented Bill with it to read.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>I gave a book to Bill to read.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>John talked to Bill about himself.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>92 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence label_predictions\n",
              "11                            She knew French for Tom.                 0\n",
              "53   She was dancing with somebody, but I don't kno...                 0\n",
              "77   I think Agnes said that Bill would speak, but ...                 0\n",
              "81                           Who did they see someone?                 0\n",
              "86                       The book was by John written.                 0\n",
              "..                                                 ...               ...\n",
              "489                      That John is reluctant seems.                 0\n",
              "493                         It is to give up to leave.                 0\n",
              "504                  I presented Bill with it to read.                 0\n",
              "505                     I gave a book to Bill to read.                 0\n",
              "515                 John talked to Bill about himself.                 0\n",
              "\n",
              "[92 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFj1kb_aIYBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gnflaleIkx8",
        "colab_type": "text"
      },
      "source": [
        "## Convert the Web Page into Sentences to Predict\n",
        "\n",
        "Source: https://requests.readthedocs.io/projects/requests-html/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLKl7Lm7NieH",
        "colab_type": "code",
        "outputId": "6582ee21-73c3-4f0b-e852-1e08d69d03df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "!pip install requests-html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting requests-html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/4b/3c2aabdd1b91fa52aa9de6cde33b488b0592b4d48efb0ad9efbf71c49f5b/pyppeteer-0.2.2-py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests-html) (2.23.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests-html) (0.0.1)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/65/220bb4075fddb09d5b3ea2c1c1fa66c1c72be9361ec187aab50fa161e576/parse-1.15.0.tar.gz\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting urllib3<2.0.0,>=1.25.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 16.2MB/s \n",
            "\u001b[?25hCollecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[?25hCollecting pyee<8.0.0,>=7.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/28/1cedd44c27907f1507a28ff2d36fc6cdb981c9deff2fa288bc48a700c7c9/pyee-7.0.2-py2.py3-none-any.whl\n",
            "Collecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.8MB/s \n",
            "\u001b[?25hCollecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html) (2.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests-html) (4.6.3)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests-html) (1.12.0)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests-html) (4.2.6)\n",
            "Building wheels for collected packages: fake-useragent, parse\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13484 sha256=691a0f24e294f870e62168b221bbd120fc8c863a2437f211d6273bce65d38628\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.15.0-cp36-none-any.whl size=23710 sha256=79a2b43bf3104f6417a9970330f829ff9eeb661b6b9c9ece22fa7de4052bcc82\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/ee/c8/eced0759f09fc635398ab1b8e89c38549b28e5db7fd4a53ba5\n",
            "Successfully built fake-useragent parse\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3, websockets, pyee, tqdm, appdirs, pyppeteer, fake-useragent, w3lib, parse, cssselect, pyquery, requests-html\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed appdirs-1.4.4 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.15.0 pyee-7.0.2 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 tqdm-4.46.1 urllib3-1.25.9 w3lib-1.22.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yotOpDjVI75n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Please type: \n",
        "\n",
        "from requests_html import HTMLSession\n",
        "session = HTMLSession()\n",
        "\n",
        "#url = \"https://searchengineland.com/the-dangers-of-misplaced-third-party-scripts-327329\"\n",
        "#now a parameter in the form above\n",
        "\n",
        "#selector=\"p > a\"\n",
        "#now a parameter in the form above\n",
        "\n",
        "with session.get(url) as r:\n",
        "\n",
        "  post = r.html.find(selector)\n",
        "\n",
        "  text = post.text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP5zq5wkNfSP",
        "colab_type": "code",
        "outputId": "4fbe2710-2502-420d-baeb-6e2e5d8f4948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was recently helping one of my team members diagnose a new prospective customer site to find some low hanging fruit to share with them.\\nWhen I checked their home page with our Chrome extension, I found a misplaced canonical tag. We added this type of detection a long time ago when I first encountered the issue.\\nWhat is a misplaced SEO tag, you might ask?\\nMost SEO tags like the title, meta description, canonical, etc. belong in the HTML HEAD. If they get placed in the HTML BODY, Google and other search engines will ignore them.\\nIf you go to the Elements tab, you will find the SEO tags inside the <BODY> tag. But, these tags are supposed to be in the <HEAD>!\\nWhy does something like this happen?\\nIf we check the page using VIEW SOURCE, the canonical tag is placed correctly inside the HTML HEAD (line 56, while the <BODY> is in line 139.).\\nWhat is happening here?!\\nIs this an issue with Google Chrome?\\nThe canonical is also placed in the BODY in Firefox.\\nWe have the same issue with Internet Explorer.\\nEdge is no exception.\\nWe have the same problem with other browsers.\\nHTML parsing vs. syntax highlighting\\nWhy is the canonical placed correctly when we check VIEW SOURCE, but not when we check it in the Elements tab?\\nIn order to understand this, I need to introduce a couple of developer concepts:\\xa0lexical analysis and syntax analysis.\\nWhen we load a source page using VIEW SOURCE, the browser automatically color codes programming tokens (HTML tags, HTML comments, etc).\\nIn order to do this, the browser performs basic lexical analysis to break the source page into HTML tokens.\\nThis task is typically performed by a lexer. It is a simple, and low-level task.\\nAll programming language compilers and interpreters use a lexer that can break source text into language tokens.\\nWhen we load the source page with the Elements tab, the browser not only does syntax highlighting, but it also builds a DOM tree.\\nIn order to build a DOM tree, it is not enough to know HTML tags and comments from regular text, you also need to know when a tag opens and closes, and their place in the tree hierarchy.\\nThis syntactic analysis requires a parser.\\nAn English spellchecker needs to perform a similar, two-phased analysis of the written text. First, it needs to translate text into nouns, pronouns, adverbs, etc. Then, it needs to apply grammar rules to make sure the part of speech tags are in the right order.\\nBut why are the SEO tags placed in the HTML body?\\nParsing HTML from Python\\nI wrote a Python script to fetch and parse some example pages with errors, find the canonical anywhere in the HTML, and print the DOM path where it was found.\\nAfter parsing the same page that shows misplaced SEO tags in the HTML Body, I find them correctly placed in the HTML head.\\nWhat are we missing?\\nInvalid tags in the HTML head\\nSome HTML tags are only valid in the HTML BODY. For example, <DIV> and <SPAN> tags are invalid in the HTML head.\\nWhen I looked closely at the HTML HEAD in our example, I found a script with a hardcoded <SPAN>. This means, the script was meant to be placed in the <BODY>, but the user incorrectly placed it in the head.\\nMaybe the instructions were not clear, the vendor omitted this information or the user didn’t know how to do this in WordPress.\\nI tested by moving the script to the BODY but still faced the misplaced canonical issue.\\nAfter a bit of trial and error, I found another script that when I moved it to the BODY, the issue disappeared.\\nWhile the second script didn’t have any hardcoded invalid tags, it was likely writing one or more to the DOM.\\nIn other words, it was doing it dynamically.\\nBut, why would inserting invalid tags, cause the browser to push the rest of the HTML in the head to the body?\\nWeb browser error tolerance\\nI created a few example HTML files with the problems I discussed and loaded them in Chrome to show you what happens.\\nIn the first example, I commented out the opening BODY tag. This removes it.\\nYou can see that Chrome added one automatically.\\nNow, let’s see what happens if I add a <DIV> inside the HTML HEAD, which is invalid.\\nThis is where it gets interesting. Chrome closed the HTML HEAD early and pushed the rest of the HEAD elements to the body, including our canonical tag and <DIV>.\\nIn other words, Chrome assumed we forgot an opening <BODY> tag!\\nThis should make it clear why misplaced tags in the HEAD can cause our SEO tags to end up in the BODY.\\nNow, let’s look at our second case where we don’t have a hardcoded invalid tag, but a script might write one dynamically.\\nHere you see that if a script writes an invalid tag in the HTML head, it will cause the browser to close it early as before. We have exactly the same problem!\\nWe didn’t see the problem with our Python parser because lxml (the Python parsing library) doesn’t try to fix HTML errors.\\nWhy do browsers do this?\\nBrowsers need to render pages that our Python script doesn’t need to do. If they try to render before correcting mistakes, the pages would look completely broken.\\nThe web is full of pages that would completely break if web browsers didn’t accommodate for errors.\\nThis article from HTML5Rocks provides a fascinating look inside web browsers and helps explain the behavior we see in our examples.\\n“The HTML5 specification does define some of these requirements. (WebKit summarizes this nicely in the comment at the beginning of the HTML parser class.)\\nUnfortunately, we have to handle many HTML documents that are not well-formed, so the parser has to be tolerant about errors.\\nWe have to take care of at least the following error conditions:\\nThe element being added is explicitly forbidden inside some outer tag. In this case, we should close all tags up to the one which forbids the element, and add it afterward.\\nPlease read the full article or at least make sure to read at least the section on “Browser’s Error Tolerance” to get a better context.\\nHow to fix this\\nFortunately, fixing this problem is actually very simple. We have two alternatives. A lazy one and a proper one.\\nThe proper fix is to track down scripts that insert invalid HTML tags in the head and move them to the HTML body.\\nThe lazy and quickest fix is to move all SEO tags (and other important tags) before any third party scripts. Preferably, right after the opening <HEAD> tag.\\nYou can see how I do it here.\\nWe still have the same invalid tag and script in the HTML head and the SEO tags are also in the head.\\nIs this a common problem?\\nI’ve been seeing this issue happening for many years now, and Patrick Stox has also reported seeing the same problem happening often to enterprise sites.\\nOne of the biggest misconceptions about technical SEO is that you do it once and you are done. That would be the case if the sites didn’t change, users/developers didn’t make mistakes and/or Googlebot behavior didn’t change either.\\nAt the moment that is hardly the case.\\nI’ve been advocating technical SEOs learn developer skills and I hope this case study illustrates the growing importance of this.\\nIf you enjoyed this tip, make sure to attend my SMX West session on Solving Complex JavaScript Issues And Leveraging Semantic HTML5 next month. Among other things, I will share advanced research on how Googlebot and Bingbot handle script and HTML issues like the ones I mentioned here.\\nOpinions expressed in this article are those of the guest author and not necessarily Search Engine Land. Staff authors are listed here.\\ngoogletag.cmd.push(function() {googletag.display(\"div-gpt-ad-ch-seo-articlemodule\"); });\\n\\n\\nAbout The Author\\nHamlet Batista\\nHamlet Batista is CEO and founder of RankSense, an agile SEO platform for online retailers and manufacturers. He holds U.S. patents on innovative SEO technologies, started doing SEO as a successful affiliate marketer back in 2002, and believes great SEO results should not take 6 months.\\nRelated Topics\\nAll Things SEO ColumnChannel: SEO'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PizYO98QPoUH",
        "colab_type": "text"
      },
      "source": [
        "###Splitting into Sentences\n",
        "Source: https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef7-ELqKPsOw",
        "colab_type": "code",
        "outputId": "46da7766-18bb-43d4-988d-c2e9f04d6009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wliiqmWeQb4R",
        "colab_type": "code",
        "outputId": "b91a5e39-88cd-49df-9fec-42afa7fce839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-CWq_FQIlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk.data\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4N9PbYhQWTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8dHkIEVNyfW",
        "colab_type": "code",
        "outputId": "6054a44f-ab27-4826-d63d-9727059efd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print ('\\n-----\\n'.join(tokenizer.tokenize(text)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I was recently helping one of my team members diagnose a new prospective customer site to find some low hanging fruit to share with them.\n",
            "-----\n",
            "When I checked their home page with our Chrome extension, I found a misplaced canonical tag.\n",
            "-----\n",
            "We added this type of detection a long time ago when I first encountered the issue.\n",
            "-----\n",
            "What is a misplaced SEO tag, you might ask?\n",
            "-----\n",
            "Most SEO tags like the title, meta description, canonical, etc.\n",
            "-----\n",
            "belong in the HTML HEAD.\n",
            "-----\n",
            "If they get placed in the HTML BODY, Google and other search engines will ignore them.\n",
            "-----\n",
            "If you go to the Elements tab, you will find the SEO tags inside the <BODY> tag.\n",
            "-----\n",
            "But, these tags are supposed to be in the <HEAD>!\n",
            "-----\n",
            "Why does something like this happen?\n",
            "-----\n",
            "If we check the page using VIEW SOURCE, the canonical tag is placed correctly inside the HTML HEAD (line 56, while the <BODY> is in line 139.).\n",
            "-----\n",
            "What is happening here?!\n",
            "-----\n",
            "Is this an issue with Google Chrome?\n",
            "-----\n",
            "The canonical is also placed in the BODY in Firefox.\n",
            "-----\n",
            "We have the same issue with Internet Explorer.\n",
            "-----\n",
            "Edge is no exception.\n",
            "-----\n",
            "We have the same problem with other browsers.\n",
            "-----\n",
            "HTML parsing vs. syntax highlighting\n",
            "Why is the canonical placed correctly when we check VIEW SOURCE, but not when we check it in the Elements tab?\n",
            "-----\n",
            "In order to understand this, I need to introduce a couple of developer concepts: lexical analysis and syntax analysis.\n",
            "-----\n",
            "When we load a source page using VIEW SOURCE, the browser automatically color codes programming tokens (HTML tags, HTML comments, etc).\n",
            "-----\n",
            "In order to do this, the browser performs basic lexical analysis to break the source page into HTML tokens.\n",
            "-----\n",
            "This task is typically performed by a lexer.\n",
            "-----\n",
            "It is a simple, and low-level task.\n",
            "-----\n",
            "All programming language compilers and interpreters use a lexer that can break source text into language tokens.\n",
            "-----\n",
            "When we load the source page with the Elements tab, the browser not only does syntax highlighting, but it also builds a DOM tree.\n",
            "-----\n",
            "In order to build a DOM tree, it is not enough to know HTML tags and comments from regular text, you also need to know when a tag opens and closes, and their place in the tree hierarchy.\n",
            "-----\n",
            "This syntactic analysis requires a parser.\n",
            "-----\n",
            "An English spellchecker needs to perform a similar, two-phased analysis of the written text.\n",
            "-----\n",
            "First, it needs to translate text into nouns, pronouns, adverbs, etc.\n",
            "-----\n",
            "Then, it needs to apply grammar rules to make sure the part of speech tags are in the right order.\n",
            "-----\n",
            "But why are the SEO tags placed in the HTML body?\n",
            "-----\n",
            "Parsing HTML from Python\n",
            "I wrote a Python script to fetch and parse some example pages with errors, find the canonical anywhere in the HTML, and print the DOM path where it was found.\n",
            "-----\n",
            "After parsing the same page that shows misplaced SEO tags in the HTML Body, I find them correctly placed in the HTML head.\n",
            "-----\n",
            "What are we missing?\n",
            "-----\n",
            "Invalid tags in the HTML head\n",
            "Some HTML tags are only valid in the HTML BODY.\n",
            "-----\n",
            "For example, <DIV> and <SPAN> tags are invalid in the HTML head.\n",
            "-----\n",
            "When I looked closely at the HTML HEAD in our example, I found a script with a hardcoded <SPAN>.\n",
            "-----\n",
            "This means, the script was meant to be placed in the <BODY>, but the user incorrectly placed it in the head.\n",
            "-----\n",
            "Maybe the instructions were not clear, the vendor omitted this information or the user didn’t know how to do this in WordPress.\n",
            "-----\n",
            "I tested by moving the script to the BODY but still faced the misplaced canonical issue.\n",
            "-----\n",
            "After a bit of trial and error, I found another script that when I moved it to the BODY, the issue disappeared.\n",
            "-----\n",
            "While the second script didn’t have any hardcoded invalid tags, it was likely writing one or more to the DOM.\n",
            "-----\n",
            "In other words, it was doing it dynamically.\n",
            "-----\n",
            "But, why would inserting invalid tags, cause the browser to push the rest of the HTML in the head to the body?\n",
            "-----\n",
            "Web browser error tolerance\n",
            "I created a few example HTML files with the problems I discussed and loaded them in Chrome to show you what happens.\n",
            "-----\n",
            "In the first example, I commented out the opening BODY tag.\n",
            "-----\n",
            "This removes it.\n",
            "-----\n",
            "You can see that Chrome added one automatically.\n",
            "-----\n",
            "Now, let’s see what happens if I add a <DIV> inside the HTML HEAD, which is invalid.\n",
            "-----\n",
            "This is where it gets interesting.\n",
            "-----\n",
            "Chrome closed the HTML HEAD early and pushed the rest of the HEAD elements to the body, including our canonical tag and <DIV>.\n",
            "-----\n",
            "In other words, Chrome assumed we forgot an opening <BODY> tag!\n",
            "-----\n",
            "This should make it clear why misplaced tags in the HEAD can cause our SEO tags to end up in the BODY.\n",
            "-----\n",
            "Now, let’s look at our second case where we don’t have a hardcoded invalid tag, but a script might write one dynamically.\n",
            "-----\n",
            "Here you see that if a script writes an invalid tag in the HTML head, it will cause the browser to close it early as before.\n",
            "-----\n",
            "We have exactly the same problem!\n",
            "-----\n",
            "We didn’t see the problem with our Python parser because lxml (the Python parsing library) doesn’t try to fix HTML errors.\n",
            "-----\n",
            "Why do browsers do this?\n",
            "-----\n",
            "Browsers need to render pages that our Python script doesn’t need to do.\n",
            "-----\n",
            "If they try to render before correcting mistakes, the pages would look completely broken.\n",
            "-----\n",
            "The web is full of pages that would completely break if web browsers didn’t accommodate for errors.\n",
            "-----\n",
            "This article from HTML5Rocks provides a fascinating look inside web browsers and helps explain the behavior we see in our examples.\n",
            "-----\n",
            "“The HTML5 specification does define some of these requirements.\n",
            "-----\n",
            "(WebKit summarizes this nicely in the comment at the beginning of the HTML parser class.)\n",
            "-----\n",
            "Unfortunately, we have to handle many HTML documents that are not well-formed, so the parser has to be tolerant about errors.\n",
            "-----\n",
            "We have to take care of at least the following error conditions:\n",
            "The element being added is explicitly forbidden inside some outer tag.\n",
            "-----\n",
            "In this case, we should close all tags up to the one which forbids the element, and add it afterward.\n",
            "-----\n",
            "Please read the full article or at least make sure to read at least the section on “Browser’s Error Tolerance” to get a better context.\n",
            "-----\n",
            "How to fix this\n",
            "Fortunately, fixing this problem is actually very simple.\n",
            "-----\n",
            "We have two alternatives.\n",
            "-----\n",
            "A lazy one and a proper one.\n",
            "-----\n",
            "The proper fix is to track down scripts that insert invalid HTML tags in the head and move them to the HTML body.\n",
            "-----\n",
            "The lazy and quickest fix is to move all SEO tags (and other important tags) before any third party scripts.\n",
            "-----\n",
            "Preferably, right after the opening <HEAD> tag.\n",
            "-----\n",
            "You can see how I do it here.\n",
            "-----\n",
            "We still have the same invalid tag and script in the HTML head and the SEO tags are also in the head.\n",
            "-----\n",
            "Is this a common problem?\n",
            "-----\n",
            "I’ve been seeing this issue happening for many years now, and Patrick Stox has also reported seeing the same problem happening often to enterprise sites.\n",
            "-----\n",
            "One of the biggest misconceptions about technical SEO is that you do it once and you are done.\n",
            "-----\n",
            "That would be the case if the sites didn’t change, users/developers didn’t make mistakes and/or Googlebot behavior didn’t change either.\n",
            "-----\n",
            "At the moment that is hardly the case.\n",
            "-----\n",
            "I’ve been advocating technical SEOs learn developer skills and I hope this case study illustrates the growing importance of this.\n",
            "-----\n",
            "If you enjoyed this tip, make sure to attend my SMX West session on Solving Complex JavaScript Issues And Leveraging Semantic HTML5 next month.\n",
            "-----\n",
            "Among other things, I will share advanced research on how Googlebot and Bingbot handle script and HTML issues like the ones I mentioned here.\n",
            "-----\n",
            "Opinions expressed in this article are those of the guest author and not necessarily Search Engine Land.\n",
            "-----\n",
            "Staff authors are listed here.\n",
            "-----\n",
            "googletag.cmd.push(function() {googletag.display(\"div-gpt-ad-ch-seo-articlemodule\"); });\n",
            "\n",
            "\n",
            "About The Author\n",
            "Hamlet Batista\n",
            "Hamlet Batista is CEO and founder of RankSense, an agile SEO platform for online retailers and manufacturers.\n",
            "-----\n",
            "He holds U.S. patents on innovative SEO technologies, started doing SEO as a successful affiliate marketer back in 2002, and believes great SEO results should not take 6 months.\n",
            "-----\n",
            "Related Topics\n",
            "All Things SEO ColumnChannel: SEO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu-e6gLeN1fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7YU6c5yO_B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sel_df = pd.DataFrame(sentences, columns=[\"sentence\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBohrt0STwOt",
        "colab_type": "code",
        "outputId": "a255ed16-8525-4895-9938-b4e3f39c33eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sel_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was recently helping one of my team members ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I checked their home page with our Chrome...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We added this type of detection a long time ag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is a misplaced SEO tag, you might ask?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Most SEO tags like the title, meta description...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence\n",
              "0  I was recently helping one of my team members ...\n",
              "1  When I checked their home page with our Chrome...\n",
              "2  We added this type of detection a long time ag...\n",
              "3        What is a misplaced SEO tag, you might ask?\n",
              "4  Most SEO tags like the title, meta description..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWBQgbepULNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncGVab2YUO62",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating their Grammar (Quality)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9iX365nURuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(sel_df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQHpZZ7EUnKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = sel_df.join(predictions)[[\"sentence\", \"label_predictions\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QmNs3duUwRo",
        "colab_type": "code",
        "outputId": "1a89a0d6-db00-48f9-b8b6-fbae332c9990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "pred_df.groupby(\"label_predictions\").count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label_predictions</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   sentence\n",
              "label_predictions          \n",
              "0                         4\n",
              "1                        85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG1xM-eUU9_v",
        "colab_type": "code",
        "outputId": "19a17ef8-5a1f-4cb4-885b-d13c2cd5a63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "pred_df[pd.to_numeric(pred_df.label_predictions) == 0]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Most SEO tags like the title, meta description...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>I tested by moving the script to the BODY but ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>In the first example, I commented out the open...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>This removes it.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             sentence label_predictions\n",
              "4   Most SEO tags like the title, meta description...                 0\n",
              "39  I tested by moving the script to the BODY but ...                 0\n",
              "45  In the first example, I commented out the open...                 0\n",
              "46                                   This removes it.                 0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUCCu9DYVL5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}